{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "#import yfinance as yf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# path to the CSV file\n",
    "file_path = (r\"C:/Users/pc/Desktop/10 Academy/Week 1/sentiment-analysis/nootbooks/data/raw_analyst_ratings.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb25a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df['headline']\n",
    "df['headline'] .nunique()\n",
    "df['headline'] .value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ff614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying sentiment to the headline\n",
    "def calculate_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# applying sentiment analysis function to the headline\n",
    "\n",
    "df['sentiment'] = df['headline'].apply(calculate_sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6aec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['headline', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74edd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "most_positive_headline = df.nlargest(5, 'sentiment')\n",
    "\n",
    "most_positive_headline[['headline', 'sentiment']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53684d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_negative_headline = df.nsmallest(5, 'sentiment')\n",
    "most_negative_headline[['headline', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfaf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_headline = df[df['sentiment'] == 0.0]\n",
    "neutral_headline[['headline', 'sentiment']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfd178f",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textual lengths (like headline length).\n",
    "\n",
    "df['headline_length'] = df['headline'].apply(len)\n",
    "df['headline_length'].describe()\n",
    "\n",
    "# count Article per publisher\n",
    "\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "publisher_counts.head()\n",
    "\n",
    "# converting to date time\n",
    "\n",
    "#df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
    "\n",
    "# count articles by date\n",
    "\n",
    "#df['date_only'] = df['date'].dt.date\n",
    "articles_per_day = df['date'].value_counts().sort_index()\n",
    "\n",
    "articles_per_day.plot(title=\"Articles Published Per Day\", figsize=(10, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05822cf9",
   "metadata": {},
   "source": [
    "Text Analysis(Topic Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12362d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "\n",
    "df['headline'] = df['headline'].replace(\"[^a-zA-Z]\", \" \", regex=True)\n",
    "df['headline'].head()\n",
    "# converting to lower case\n",
    "df['headline'] = df['headline'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ecd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting headline to lower case\n",
    "new_Index = ['headline']  \n",
    "\n",
    "for index in new_Index:\n",
    "    df[index] = df[index].str.lower()\n",
    "df['headline'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "stop_words = set(STOPWORDS)\n",
    "\n",
    "# Join all headlines into one string\n",
    "text = ' '.join(df['headline'].astype(str))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d03d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Download stopwords \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# # Define stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Join all headlines into one string\n",
    "# text = ' '.join(df['headline'].astype(str))\n",
    "\n",
    "# # Generate the word cloud\n",
    "# wordcloud = WordCloud(stopwords=stop_words, background_color='white').generate(text)\n",
    "\n",
    "# # Plot it\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aacc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load CSV\n",
    "file_path = r\"C:/Users/pc/Desktop/10 Academy/Week 1/sentiment-analysis/nootbooks/data/raw_analyst_ratings.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Optional: drop rows with missing headlines\n",
    "df = df.dropna(subset=['headline'])\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each headline with spaCy\n",
    "processed_docs = [nlp(headline) for headline in df['headline'].astype(str)]\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Show elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Processing time for {len(processed_docs)} headlines: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5834dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['publisher'] = pd.to_datetime(df['publisher'], errors='coerce')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90730193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_only'] = df['publisher'].dt.date\n",
    "daily_counts = df.groupby('date_only').size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "daily_counts.plot()\n",
    "plt.title('Daily Article Publication Frequency')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd35bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['hour'].value_counts().sort_index().plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Articles by Hour of the Day\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['publisher'].isnull().sum())\n",
    "print(df['publisher'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "# Extract hour\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Count articles by hour\n",
    "hourly_counts = df['hour'].value_counts().sort_index()\n",
    "hourly_counts.plot(kind='bar', figsize=(10,5), color='skyblue')\n",
    "plt.title(\"Articles by Hour of the Day\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv((file_path), parse_dates=['date'])  # Only parse 'date' column as datetime\n",
    "print(df['publisher'].head())  # Should show strings (publisher names)\n",
    "\n",
    "print(df['publisher'].head())  # or use the correct column name from step 1\n",
    "print(df['publisher'].value_counts())\n",
    "print(df.columns)               # Check all column names\n",
    "print(df['publisher'].head())  # Check first few rows of 'publisher' column\n",
    "print(df['publisher'].value_counts())  # Check counts for unique publishers\n",
    "print(df['publisher'].dtype)\n",
    "print(df['publisher'].nunique())\n",
    "print(df['publisher'].isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_publisher = df['publisher'].value_counts().head(10)\n",
    "top_publisher.plot(kind='bar', figsize=(10,5), color='skyblue')\n",
    "plt.title(\"Top 10 Publishers\")\n",
    "plt.xlabel(\"Publisher\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33459813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=df, x='publisher', hue='stock')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of Articles by Publisher by Stock\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['domain'] = df['publisher'].str.extract(r'@([\\w\\.-]+)')\n",
    "df['domain'].value_counts().head(10).plot(kind='bar', figsize=(10,5))\n",
    "plt.title(\"Top Email Domains (Organizations)\")\n",
    "plt.xlabel(\"Domain\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(\" \".join(df['headline'].dropna()), \n",
    "                                     keyphrase_ngram_range=(1, 2), \n",
    "                                     stop_words='english', \n",
    "                                     top_n=20)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['headline'].dropna())\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([words[i] for i in topic.argsort()[:-11:-1]]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
